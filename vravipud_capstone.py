# -*- coding: utf-8 -*-
"""vravipud_capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxbjxe6urtjvos2kmDFefeyLBixdMkHB

## <center>Heart Attack Analysis & Prediction</center>

##Introduction:
The objective of this project is to undertake a comprehensive analysis of a dataset containing pertinent information relating to heart attack occurrences. Through the construction of a predictive model, the intent is to ascertain the probability of heart attack incidence based on parameters such as age, gender, chest pain type, blood pressure, cholesterol levels, and other associated factors. This investigation aims to elucidate the complex interplay between these factors and the occurrence of heart attacks, thereby contributing to early detection and prevention methodologies.

## Learing outcomes from this project
* EDA
* Missing Value Analysis
* Categoric and Numeric Features
* Standardization
* Box - Swarm - Cat - Correlation Plot Analysis
* Outlier Detection
* Modelling and Tuning Machine Learning Model

## Analysis Content
1. [Python Libraries](#1)
1. [Data Content](#2)
1. [Read and Analyse Data](#3)
1. [Missing Value Analysis](#4)
1. [Unique Value Analysis](#5)
1. [Categorical Feature Analysis](#6)
1. [Numeric Feature Analysis](#7)
1. [Standardization](#8)
1. [Box Plot Analysis](#9)
1. [Swarm Plot Analysis](#10)
1. [Cat Plot Analysis](#11)
1. [Correlation Analysis](#12)
1. [Outlier Detection](#13)
1. [Modelling](#14)
    1. Encoding Categorical Columns
    1. Scaling
    1. Train/Test Split
    1. Logistic Regression

<a id="1"></a>
## Python Libraries
* In this section, we import used libraries during this kernel.
"""

#import Libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, roc_curve
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings("ignore")

"""<a id="2"></a>
## Data Content
* **Age:** Age of the patient
* **Sex:** Sex of the patient
* **exang:** exercise induced angina (1 = yes; 0 = no)
* **ca:** number of major vessels (0-3)
* **cp:** Chest Pain type chest pain type
    * Value 1: typical angina
    * Value 2: atypical angina
    * Value 3: non-anginal pain
    * Value 4: asymptomatic
* **trtbps:** resting blood pressure (in mm Hg)
* **chol:** cholestoral in mg/dl fetched via BMI sensor
* **fbs:** (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
* **rest_ecg:** resting electrocardiographic results
    * Value 0: normal
    * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
* **thalach:** maximum heart rate achieved
* **target:** 0= less chance of heart attack 1= more chance of heart attack

<a id="3"></a>
## Read and Analyse Data
* In this section, we read heart.csv
"""

# read data
df = pd.read_csv("/content/heart.csv")

df.head()

# describe basic statistics of data
df.describe().T

# information about dataframe
df.info()

"""<a id="4"></a>
## Missing Value Analysis
"""

# missing value
df.isnull().sum().sum()

"""<a id="5"></a>
## Unique Value Analysis
"""

for i in df.columns:
    print(f"{i} -- {df[i].value_counts().shape[0]}")

"""<a id="6"></a>
## Categorical Feature Analysis
"""

def grab_col_names(dataframe, cat_th=10, car_th=20):

    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]
    return cat_cols, num_cols, cat_but_car

help(grab_col_names)

cat_cols, num_cols, cat_but_car = grab_col_names(df)

print(f"Observations: {df.shape[0]}")
print(f"Variables: {df.shape[1]}")
print(f'cat_cols: {len(cat_cols)}')
print(f'num_cols: {len(num_cols)}')
print(f'cat_but_car: {len(cat_but_car)}')
print(f'num_but_cat: {len([col for col in cat_cols if col in num_cols])}')

import matplotlib.pyplot as plt
import seaborn as sns

# Select categorical columns
df_categoric = df.loc[:, cat_cols]

# Set the dimensions for the plots
fig_width = 10  # Width of the figure
fig_height = 6  # Height of the figure

# Calculate the number of rows and columns needed for the grid
num_rows = 3
num_cols = 3
num_plots = min(len(cat_cols), num_rows * num_cols)

# Create subplots with the desired grid layout
fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))
axes = axes.flatten()

# Loop through categorical columns and create count plots
for i in range(num_plots):
    sns.countplot(x=cat_cols[i], data=df_categoric, hue="output", ax=axes[i])
    axes[i].set_title(cat_cols[i])

    # Save the plot with custom dimensions
    plt.savefig(f"{cat_cols[i]}_countplot.png", dpi=300, bbox_inches="tight")  # Adjust dpi and bbox_inches as needed

# Adjust layout spacing
plt.tight_layout()

# Display the plots
plt.show()

"""<a id="7"></a>
## Numeric Feature Analysis
* Bivariate data analysis with scatter plot
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the dimensions for the output graph
graph_height = 10  # Height of the graph
graph_width = 10   # Width of the graph

# Select numeric columns
num_cols = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak', 'output']
df_numeric = df.loc[:, num_cols]

# Create the pair plot
g = sns.pairplot(df_numeric, hue="output", diag_kind="kde")

# Set the dimensions of the graph
g.fig.set_figheight(graph_height)
g.fig.set_figwidth(graph_width)

# Display the graph
plt.show()

"""* There are numeric values in the row and column. Orange and gray dots show output.

* If we look at the intersection of the Oldpeak variable, there is a left slant, which means positive skewness.
* Thalacahm, on the other hand, has negative skewness as it is tilted to the right.

* We can comment on the correlation by looking at the distribution of the numerical variables in the table with each other.

* If the image of the dots looks round, it is understood that there is no correlation.

<a id="8"></a>
## Standardization

* Standardization is the process of scaling data and rescaling the values in a data set according to a specific mean and standard deviation. This process is necessary to make accurate comparisons between variables measured in different units.

* Having the features in the data set at the same scale helps the model to give more accurate results.
"""

scaler = StandardScaler()
scaler

scaled_array = scaler.fit_transform(df[num_cols[:-1]])

scaled_array

pd.DataFrame(scaled_array).describe().T

"""<a id="9"></a>
## Box Plot Analysis
"""

df_dummy = pd.DataFrame(scaled_array, columns = num_cols[:-1])
df_dummy.head()

df_dummy = pd.concat([df_dummy, df.loc[:,"output"]], axis = 1)
df_dummy.head()

data_melted = pd.melt(df_dummy,
                      id_vars = "output",
                      var_name = "features",
                      value_name = "value")
data_melted.head(20)

# box.plot
plt.figure()
sns.boxplot(x = "features",
            y = "value",
            hue = "output",
            data = data_melted)
plt.show()

"""* You can see outliers with boxplot visualization

<a id="10"></a>
## Swarm Plot Analysis

* Swarm plot analysis is used to visualize the distribution and density of data.

* This type of chart is especially useful in small datasets and helps to easily detect outliers in the dataset thanks to its point distribution. Moreover, the swarm plot can also be used to visualize relationships between different groups in the dataset.
"""

# swarm plot
plt.figure()
sns.swarmplot(x = "features",
              y = "value",
              hue = "output",
              data= data_melted)
plt.show()

"""<a id="11"></a>
## Cat Plot Analysis
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the dimensions for the output graph
graph_height = 10  # Height of the graph
graph_width = 8   # Width of the graph

# Create the catplot with a swarm plot
g = sns.catplot(x="exng", y="age", hue="output", col="sex", kind="swarm", data=df)

# Set the dimensions of the graph
g.fig.set_figheight(graph_height)
g.fig.set_figwidth(graph_width)

# Display the graph
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set the dimensions for the output graph
graph_height = 6  # Height of the graph
graph_width = 4   # Width of the graph

# Create the catplot
g = sns.catplot(x="exng", y="age", hue="output", col="sex", kind="strip", data=df)

# Set the dimensions of the graph
g.fig.set_figheight(graph_height)
g.fig.set_figwidth(graph_width)

# Display the graph
plt.show()

"""* Strip plot and swarm plot are both charts used to visualize the relationship of a categorical variable to a numerical variable. However, a strip plot displays data on only one line, while a swarm plot displays data as overlapped points.

* A swarm plot is similar to a strip plot, but its points are offset so that the data can be better viewed, thus avoiding overlapping of the data.

* Therefore, the swarm plot can be better read and help better understand the data. However, for large datasets the strip plot may be faster and more convenient because the swarm plot takes up more space and requires more computation.

<a id="12"></a>
## Correlation Analysis
"""

plt.figure(figsize = (14,10))
sns.heatmap(df.corr(), annot = True, fmt = ".1f", linewidths = .7)
plt.show()

"""<a id="13"></a>
## Outlier Detection
* Outliers can disrupt ML process.
"""

num_cols

def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

outlier_thresholds(df, "output")

"""* Write a function that prints true if there is an outlier between numeric variables and false otherwise."""

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

for col in num_cols:
    print(col, check_outlier(df, col))

"""* Let's write a function that finds outliers themselves.
* If we want to see the index if there is an outlier, we can do this by typing inplace = true.
"""

def grab_outliers(dataframe, col_name, index=False):
    low, up = outlier_thresholds(dataframe, col_name)

    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:
        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())
    else:
        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])

    if index:
        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index
        return outlier_index

grab_outliers(df, "output")

grab_outliers(df,"oldpeak","True")

"""
## Re-Assignment with Threshold
"""

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

cat_cols, num_cols, cat_but_car = grab_col_names(df)

for col in num_cols:
    print(col, check_outlier(df, col))

for col in num_cols:
    replace_with_thresholds(df, col)

for col in num_cols:
    print(col, check_outlier(df, col))

"""* No more outliers. As you can see, the output of the code is "False".

<a id="14"></a>
## Modelling
"""

df1 = df.copy()

"""### Encoding Categorical Columns

* Note : With get_dummies we can do both binary encoding and one-hot encoding.
"""

def one_hot_encoder(dataframe, categorical_cols, drop_first=True):
    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
    return dataframe

ohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]
ohe_cols

one_hot_encoder(df, ohe_cols).head()

X = df1.drop(["output"], axis = 1)
y = df1[["output"]]

"""### Scaling

* StandardScaler is a preprocessing technique in Python used for standardizing the features of a dataset.
* It transforms the dataset such that its distribution has a mean of 0 and a standard deviation of 1. This transformation is applied independently to each feature in the dataset.
"""

scaler = StandardScaler()
scaler

X[num_cols[:-1]] = scaler.fit_transform(X[num_cols[:-1]])
X.head()

"""### Train/Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train: {}".format(X_train.shape))
print("X_test: {}".format(X_test.shape))
print("y_train: {}".format(y_train.shape))
print("y_test: {}".format(y_test.shape))

"""### Logistic Regression

* Logistic regression is a statistical method used to analyze a dataset in which there are one or more independent variables that determine an outcome.

* "The purpose of logistic regression is to model the relationship between the dependent and independent variables in a classification problem and create a linear model to predict the outcome. The predicted outcome is given as a probability value for each input feature, and classification is typically done by setting a threshold value."
"""

log_model = LogisticRegression().fit(X, y)

# calculate probabilities
y_pred_prob = log_model.predict_proba(X_test)
y_pred_prob

y_pred = np.argmax(y_pred_prob, axis = 1)
y_pred

y_test

dummy_ = pd.DataFrame(y_pred_prob)
dummy_["y_pred"] = y_pred
dummy_.head()

def plot_confusion_matrix(y_test, y_pred):
    acc = round(accuracy_score(y_test, y_pred), 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt=".0f")
    plt.xlabel('y_pred')
    plt.ylabel('y_test')
    plt.title('Accuracy Score: {0}'.format(acc), size=10)
    plt.show()

plot_confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

"""#### Reciever Operating Characteristic Curve (ROC Curve)

"""

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])

# plot curve
plt.plot([0,1],[0,1],"k--")
plt.plot(fpr, tpr, label = "Logistic Regression")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Logistic Regression ROC Curve")
plt.show()

from sklearn.metrics import roc_auc_score

y_prob = log_model.predict_proba(X)[:, 1]
roc_auc_score(y, y_prob)

"""##Conclusion
In this project, I created a computer program (a logistic regression model) to predict who might have heart disease. The result was a success, scoring 0.9249 on a scale that goes up to 1. This score shows that the program is excellent at telling the difference between people with heart disease and those without.

I started by looking at different health factors that might be linked to heart disease. Then, I used this information to build a model that could help doctors find and treat heart disease earlier.

The steps I took to prepare and analyze the data were key to making the model work well. Although the current model is effective, there's room to make it even better in the future by exploring other methods and making some adjustments.

Overall, this project highlights how computer technology, specifically machine learning, can be used in healthcare. It's a clear example of how these tools can be put into practical use to potentially save lives.
"""